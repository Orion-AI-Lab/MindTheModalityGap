# [Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment](https://arxiv.org/abs/2402.09816)

### Pretrained models
The weights of both aligned and patched models can be accessed using the following links:
  - (Patched)[https://mega.nz/folder/z2wwmLDS#d-ketrlbZFI9hqdjSW2AQA]
  - (Aligned)[https://mega.nz/folder/erR0wZTQ#wQRpE8r8RRkNtK9BTlHYDA]

### Citation
If you use this work please cite:
```
@misc{zavras2024mindmodalitygapremote,
      title={Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment}, 
      author={Angelos Zavras and Dimitrios Michail and Beg√ºm Demir and Ioannis Papoutsis},
      year={2024},
      eprint={2402.09816},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.09816}, 
}
```